{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.0 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "9a984502b4f22417653ed0ad7abc5ee7716c5fb39121ab29863ffc014419618f"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[MLENS] backend: threading\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Ensemble Learning on Adult Dataset.\n",
    "\n",
    "@2020 Created by Vihang Garud.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Importing required libraries\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from tabulate import tabulate\n",
    "\n",
    "from mlens.ensemble import SuperLearner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Importing the datasets\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Adding column names\n",
    "column_names = ['Age', 'Work_Class', 'Fnlwgt', 'Education', 'Education_Num', 'Marital_Status', 'Occupation',\n",
    "                'Relationship', 'Race', 'Sex', 'Capital_Gain', 'Capital_Loss', 'Hours_per_week', 'Native_Country',\n",
    "                'Salary']\n",
    "\n",
    "# Importing the datasets\n",
    "train_data = pd.read_csv('adult.data', names=column_names)\n",
    "test_data = pd.read_csv('adult.test', skiprows=1, names=column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Preprocessing the train dataset\n",
    "\n",
    "- Dropping unnecessary columns\n",
    "- Mapping binary valued columns\n",
    "- Dropping missing values\n",
    "- One Hot Encoding\n",
    "\n",
    "\"\"\"\n",
    "# Drop education num and native country column\n",
    "train_data.drop(['Education_Num', 'Native_Country'], axis=1, inplace=True)\n",
    "\n",
    "# Map binary valued columns\n",
    "train_data['Salary'] = train_data['Salary'].map({' <=50K': 0, ' >50K': 1})\n",
    "train_data['Sex'] = train_data['Sex'].map({' Female': 0, ' Male': 1})\n",
    "\n",
    "# Drop missing value instances\n",
    "train_data = train_data.replace({' ?': np.nan}).dropna()\n",
    "\n",
    "# One Hot Encoding for categorical columns\n",
    "train_data = pd.get_dummies(train_data)\n",
    "\n",
    "# Shifting class label\n",
    "salary = train_data.pop('Salary')\n",
    "train_data['Salary'] = salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Preprocessing the test dataset\n",
    "\n",
    "- Dropping unnecessary columns\n",
    "- Mapping binary valued columns\n",
    "- Dropping missing values\n",
    "- One Hot Encoding\n",
    "\n",
    "\"\"\"\n",
    "# Drop education num and native country column\n",
    "test_data.drop(['Education_Num', 'Native_Country'], axis=1, inplace=True)\n",
    "\n",
    "# Map binary valued columns\n",
    "test_data['Salary'] = test_data['Salary'].map({' <=50K.': 0, ' >50K.': 1})\n",
    "test_data['Sex'] = test_data['Sex'].map({' Female': 0, ' Male': 1})\n",
    "\n",
    "# Drop missing value instances\n",
    "test_data = test_data.replace({' ?': np.nan}).dropna()\n",
    "\n",
    "# One Hot Encoding for categorical columns\n",
    "test_data = pd.get_dummies(test_data)\n",
    "\n",
    "# Shifting class label\n",
    "salary = test_data.pop('Salary')\n",
    "test_data['Salary'] = salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Readying train and test sets\n",
    "\n",
    "X_train = train_data.iloc[:, : -1]\n",
    "Y_train = train_data.iloc[:, -1]\n",
    "X_test = test_data.iloc[:, : -1]\n",
    "Y_test = test_data.iloc[:, -1]"
   ]
  },
  {
   "source": [
    "## Individual Classifiers"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Calculating Accuracies for individual classifiers\n",
    "\n",
    "- KNeighbors Classifier\n",
    "- Naive Bayes Classifier\n",
    "- Support Vector Classifier\n",
    "- Decision Tree Classifier\n",
    "- Random Forest Classifier\n",
    "- Adaboost Classifier\n",
    "- Gradient Boosting Classifier\n",
    "- Linear Discriminant Analysis Classifier\n",
    "- Multilayer Perceptron Classifier\n",
    "- Logistic Regression Classifier\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# KNeighbors Classifier Predictions\n",
    "\n",
    "knn_clf = KNeighborsClassifier(n_neighbors=15, weights='distance', p=1)\n",
    "knn_clf.fit(X_train, Y_train)\n",
    "knn_Y_pred = knn_clf.predict(X_test)\n",
    "\n",
    "# Naive Bayes Classifier Predictions\n",
    "\n",
    "nb_clf = GaussianNB()\n",
    "nb_clf.fit(X_train, Y_train)\n",
    "nb_Y_pred = nb_clf.predict(X_test)\n",
    "\n",
    "# Support Vector Classifier Predictions\n",
    "\n",
    "sv_clf = SVC(C=1000)\n",
    "sv_clf.fit(X_train, Y_train)\n",
    "sv_Y_pred = sv_clf.predict(X_test)\n",
    "\n",
    "# Decision Tree Classifier Predictions\n",
    "\n",
    "dt_clf = DecisionTreeClassifier(criterion='entropy', min_samples_split=300, max_leaf_nodes=200)\n",
    "dt_clf.fit(X_train, Y_train)\n",
    "dt_Y_pred = dt_clf.predict(X_test)\n",
    "\n",
    "# Random Forest Classifier Predictions\n",
    "\n",
    "rf_clf = RandomForestClassifier(n_estimators=300, min_samples_split=400, n_jobs=100)\n",
    "rf_clf.fit(X_train, Y_train)\n",
    "rf_Y_pred = rf_clf.predict(X_test)\n",
    "\n",
    "# Adaboost Classifier Predictions\n",
    "\n",
    "ada_clf = AdaBoostClassifier(n_estimators=400)\n",
    "ada_clf.fit(X_train, Y_train)\n",
    "ada_Y_pred = ada_clf.predict(X_test)\n",
    "\n",
    "# Gradient Boosting Classifier Predictions\n",
    "\n",
    "gb_clf = GradientBoostingClassifier(learning_rate=0.9, min_samples_split=400)\n",
    "gb_clf.fit(X_train, Y_train)\n",
    "gb_Y_pred = gb_clf.predict(X_test)\n",
    "\n",
    "# Linear Discriminant Analysis Classifier Predictions\n",
    "\n",
    "lda_clf = LinearDiscriminantAnalysis()\n",
    "lda_clf.fit(X_train, Y_train)\n",
    "lda_Y_pred = lda_clf.predict(X_test)\n",
    "\n",
    "# Multilayer Perceptron Classifier Predictions\n",
    "\n",
    "mlp_clf = MLPClassifier()\n",
    "mlp_clf.fit(X_train, Y_train)\n",
    "mlp_Y_pred = mlp_clf.predict(X_test)\n",
    "\n",
    "# Logistic Regression Classifier Predictions\n",
    "\n",
    "lr_clf = LogisticRegression(solver='liblinear', penalty='l1')\n",
    "lr_clf.fit(X_train, Y_train)\n",
    "lr_Y_pred = lr_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+---------------------+----------------+\n| Algorithms          |   Accuracy (%) |\n|---------------------+----------------|\n| K-Neighbors         |        79.3275 |\n| Naive Bayes         |        78.8116 |\n| SVM                 |        79.223  |\n| Decision Tree       |        84.9429 |\n| Random Forest       |        85.7525 |\n| Adaboost            |        86.6732 |\n| Gradient Boosting   |        86.3532 |\n| LDA                 |        83.8459 |\n| MLP                 |        77.016  |\n| Logistic Regression |        84.8253 |\n+---------------------+----------------+\n"
     ]
    }
   ],
   "source": [
    "# Printing all the classifier accuracies\n",
    "\n",
    "print(tabulate([['K-Neighbors', accuracy_score(Y_test, knn_Y_pred) * 100],\n",
    "                ['Naive Bayes', accuracy_score(Y_test, nb_Y_pred) * 100],\n",
    "                ['SVM', accuracy_score(Y_test, sv_Y_pred) * 100],\n",
    "                ['Decision Tree', accuracy_score(Y_test, dt_Y_pred) * 100],\n",
    "                ['Random Forest', accuracy_score(Y_test, rf_Y_pred) * 100],\n",
    "                ['Adaboost', accuracy_score(Y_test, ada_Y_pred) * 100],\n",
    "                ['Gradient Boosting', accuracy_score(Y_test, gb_Y_pred) * 100],\n",
    "                ['LDA', accuracy_score(Y_test, lda_Y_pred) * 100],\n",
    "                ['MLP', accuracy_score(Y_test, mlp_Y_pred) * 100],\n",
    "                ['Logistic Regression', accuracy_score(Y_test, lr_Y_pred) * 100]],\n",
    "                ['Algorithms', 'Accuracy (%)'], 'psql'))"
   ]
  },
  {
   "source": [
    "## Performing Ensemble Learning"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "SuperLearner(array_check=None, backend=None, folds=10,\n",
       "       layers=[Layer(backend='threading', dtype=<class 'numpy.float32'>, n_jobs=-1,\n",
       "   name='layer-1', propagate_features=None, raise_on_exception=True,\n",
       "   random_state=0, shuffle=True,\n",
       "   stack=[Group(backend='threading', dtype=<class 'numpy.float32'>,\n",
       "   indexer=FoldIndex(X=None, folds=10, raise_on_excep...A54E0D0>)],\n",
       "   n_jobs=-1, name='group-1', raise_on_exception=True, transformers=[])],\n",
       "   verbose=0)],\n",
       "       model_selection=False, n_jobs=None, raise_on_exception=True,\n",
       "       random_state=0, sample_size=30718,\n",
       "       scorer=<function accuracy_score at 0x000001A4FA54E0D0>,\n",
       "       shuffle=True, verbose=False)"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "\"\"\"\n",
    "Performing ensemble learning\n",
    "\n",
    "- Creating a model list\n",
    "- Creating the ensembler\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "models = []\n",
    "models.append(KNeighborsClassifier(n_neighbors=15, weights='distance', p=1))\n",
    "models.append(GaussianNB())\n",
    "models.append(SVC(C=1000))\n",
    "models.append(DecisionTreeClassifier(criterion='entropy', min_samples_split=300, max_leaf_nodes=200))\n",
    "models.append(RandomForestClassifier(n_estimators=300, min_samples_split=400, n_jobs=100))\n",
    "models.append(AdaBoostClassifier(n_estimators=400))\n",
    "models.append(GradientBoostingClassifier(learning_rate=0.9, min_samples_split=400))\n",
    "models.append(LinearDiscriminantAnalysis())\n",
    "models.append(MLPClassifier())\n",
    "models.append(LogisticRegression(solver='liblinear', penalty='l1'))\n",
    "\n",
    "ensemble = SuperLearner(scorer=accuracy_score, folds=10, shuffle=True, sample_size=len(X_train), random_state=0)\n",
    "ensemble.add(models)\n",
    "ensemble.add_meta(AdaBoostClassifier(n_estimators=400))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy using Ensemble Learning : 86.78 %\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Performing ensemble learning\n",
    "\n",
    "- Fitting the ensemble model\n",
    "- Predicting the values\n",
    "- Printing the accuracy of the ensemble model\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "ensemble.fit(X_train.values, Y_train.values)\n",
    "ensemble_Y_pred = ensemble.predict(X_test.values)\n",
    "\n",
    "print('Accuracy using Ensemble Learning : {:.2f}'.format(accuracy_score(Y_test, ensemble_Y_pred) * 100), '%')"
   ]
  }
 ]
}